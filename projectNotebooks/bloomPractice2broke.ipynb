{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c82993-e824-41e6-9798-05a0bb9020a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os.path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BloomConfig\n",
    "from transformers.models.bloom.modeling_bloom import BloomBlock, build_alibi_tensor\n",
    "\n",
    "def get_state_dict(shard_num, prefix=None):\n",
    "    d = torch.load(model, f\"pytorch_model_{shard_num:05d}-of-00072.bin\")\n",
    "    return d if prefix is None else OrderedDict((k.replace(prefix, ''), v) for k, v in d.items())\n",
    "\n",
    "model_path = os.path.abspath(os.path.join(os.sep, 'bloom'))\n",
    "config = BloomConfig.from_pretrained(model_path)\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(model_path)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce9865-2f41-4680-ab52-8a9eaa80fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 methods to load state dictionaries into different objects. This loads only specific parts to RAM to save memory.\n",
    "def load_embeddings():\n",
    "    state_dict = get_state_dict(shard_num=1, prefix=\"word_embeddings_layernorm.\")\n",
    "    embeddings = nn.Embedding.from_pretrained(state_dict.pop('word_embeddings.weight'))\n",
    "    lnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, dtype=torch.bfloat16)\n",
    "    lnorm.load_state_dict(state_dict)\n",
    "    return embeddings.to(device), lnorm.to(device)\n",
    "\n",
    "def load_causal_lm_head():\n",
    "    linear = nn.utils.skip_init(\n",
    "        nn.Linear, config.hidden_size, config.vocab_size, bias=False, dtype=torch.bfloat16)\n",
    "    linear.load_state_dict(get_state_dict(shard_num=1, prefix=\"word_embeddings.\"), strict=False)\n",
    "    return linear.bfloat16().to(device)\n",
    "\n",
    "def load_block(block_obj, block_num):\n",
    "    block_obj.load_state_dict(get_state_dict(shard_num=block_num + 2, prefix=f\"h.{block_num}.\"))\n",
    "    block_obj.to(device)\n",
    "\n",
    "final_lnorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, dtype=torch.bfloat16)\n",
    "final_lnorm.load_state_dict(get_state_dict(shard_num=72, prefix=\"ln_f.\"))\n",
    "final_lnorm.to(device)\n",
    "block = BloomBlock(config, layer_number=1).bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d265e79-d625-4525-8be9-1871f422cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input_ids):\n",
    "    # 1. Create attention mask and position encodings\n",
    "    attention_mask = torch.ones(len(input_ids)).unsqueeze(0).bfloat16().to(device)\n",
    "    alibi = build_alibi_tensor(input_ids.shape[1], config.num_attention_heads,\n",
    "                               torch.bfloat16).to(device)\n",
    "    # 2. Load and use word embeddings\n",
    "    embeddings, lnorm = load_embeddings()\n",
    "    hidden_states = lnorm(embeddings(input_ids))\n",
    "    del embeddings, lnorm\n",
    "\n",
    "    # 3. Load and use the BLOOM blocks sequentially\n",
    "    for block_num in range(70):\n",
    "        load_block(block, block_num)\n",
    "        hidden_states = block(hidden_states, attention_mask=attention_mask, alibi=alibi)[0]\n",
    "        print(\".\", end='')\n",
    "    \n",
    "    hidden_states = final_lnorm(hidden_states)\n",
    "    \n",
    "    #4. Load and use language model head\n",
    "    lm_head = load_causal_lm_head()\n",
    "    logits = lm_head(hidden_states)\n",
    "\n",
    "    # 5. Compute next token \n",
    "    return torch.argmax(logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453b2d8-8d26-4727-bb90-a5bb31e36d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"The SQL command to extract all the users whose name starts with A is: \"\n",
    "input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "max_tokens = 10\n",
    "for i in range(max_tokens): \n",
    "    print(f\"Token {i + 1} \", end='')\n",
    "    new_id = forward(input_ids)\n",
    "    input_ids = torch.cat([input_ids, new_id.unsqueeze(-1)], dim=-1)\n",
    "    print(tokenizer.decode(new_id))\n",
    "\n",
    "print(tokenizer.decode(input_ids.squeeze(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
